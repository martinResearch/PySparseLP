import copy
import numpy as np
import time
from scipy import sparse
import scipy.ndimage
from tools import preconditionConstraints,convertToStandardFormWithBounds,chrono,check_decrease
import  scikits.sparse.cholmod

def LP_admmBlockDecomposition(c,Aeq,beq,Aineq,b_lower,b_upper,lb,ub,x0=None,gamma_ineq=0.7,nb_iter=100,callbackFunc=None,max_time=None,use_preconditionning=True,useLU=True,nb_iter_plot=10):
	# simple ADMM method with an approximate resolutio of a quadratic subproblem using conjugate gradient
	# inspiredy by Boyd's paper on ADMM
	# Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers
	# the difference with LP_admm is that the linear quality constrainrs Aeq*beq are enforced during the resolution 
	# of the subproblem instead of beeing enforced through multipliers 
	n=c.size
	start = time.clock() 				
	elapsed=start	
	if x0==None:
		x0=np.zeros(c.size)
	#if Aeq!=None:
		#Aeq,beq=preconditionConstraints(Aeq,beq,alpha=2)
	#if Aineq!=None:# it seem important to do this preconditionning before converting to standard form
		#Aineq,b_lower,b_upper=preconditionConstraints(Aineq,b_lower,b_upper,alpha=2)

	c,Aeq,beq,lb,ub,x0=	convertToStandardFormWithBounds(c,Aeq,beq,Aineq,b_lower,b_upper,lb,ub,x0)
	x=x0

	xp=x.copy()
	xp=np.maximum(xp,lb)
	xp=np.minimum(xp,ub)	

	#trying some preconditioning
	use_preconditionning_rows=False # left preconditioning seems not to change anything if not used in combination with use_preconditionning_cols  as each subproblem is solved exactly.
	if use_preconditionning_rows:
		Aeq,beq=preconditionConstraints(Aeq,beq,alpha=2)
		
	#global right preconditioning
	use_preconditionning_cols=False 
	if use_preconditionning_cols:
		R,c,Aeq,beq,lb,ub,x0=preconditionLPRight(c,Aeq,beq,lb,ub,x0,alpha=3)
	else:
		R=sparse.eye(Aeq.shape[1])

	luMs=[]
	nb_used=np.zeros(x.shape)
	list_block_ids=[]
	#Aeq.blocks=[(0,Aeq.blocks[5][1]),(Aeq.blocks[6][0],Aeq.blocks[11][1])]	
	#Aeq.blocks=[(0,Aeq.blocks[-1][1])]	
	
	beqs=[]
	usesparseLU=False
	xv=[]
	#for idblock in range(nb_blocks):
	ch=chrono()	
	
	mergegroupes=[]
	mergegroupes=[[k] for k in range(len(Aeq.blocks))]
	#mergegroupes.append(np.arange(len(Aeq.blocks)))#single block
	
	#mergegroupes.append([0,1,2,3, 4, 5])
	#mergegroupes.append([6,7,8,9,10,11])
	#mergegroupes.append([12,13,14])
	#mergegroupes.append([0,1,2,3, 4, 5,12])
	#mergegroupes.append([6,7,8,9,10,11,13])
	#mergegroupes.append([14])	
	
	#mergegroupes.append([0,1,2,3,4,5,12,13,])
	#mergegroupes.append([6,7,8,9,10,11,14,15])
	#mergegroupes.append([0,1,2,3,4,5])
	#mergegroupes.append([6,7,8,9,10,11])
	#mergegroupes.append([12,13])
	#mergegroupes.append([14,15])
	#mergegroupes.append([12,13,14,15])
	nb_blocks=len( mergegroupes)
	
	if False:
		idRows=np.hstack([np.arange(Aeq.blocks[g][0],Aeq.blocks[g][1]+1) for g in mergegroupe])
		
		# we want to cluster the constraints such that the submatrix for each cluster has a sparse cholesky decomposition
		# we need to reach a tradeoff between the number of cluster (less variables copies) and the sparsity of the cholesky decompositions
		# each cluster should have a small tree width ? 
		# can we do an incremental sparse cholseky an then  add one constraint to each cholesky at a time ? 
		# is non sparse cholseky decomposition  (without permutation)incremental ? 
		# adding a factor to a block is good if does not add too many new variables to the block and does not augment the treewidth of the block ? 
		# we can do incremental merging of blocks
		# marge is good if the interection of the variables used by the two block is large (remove copies) (fast to test)
		# and if the operation does not increase the cholseky density or graph tree width too much 
		
		subA=Aeq[idRows,:]
		t=np.array(np.abs(subA).sum(axis=0)).ravel()
		ids=np.nonzero(t)[0]
		list_block_ids.append(ids)
	
		subA2=subA[:,ids]
		# precompute the LU factorizartion of the matrix that needs to be inverted for the block
		M=scipy.sparse.vstack((scipy.sparse.hstack((gamma_ineq*scipy.sparse.eye(subA2.shape[1],subA2.shape[1]),subA2.T)),\
			                       scipy.sparse.hstack((subA2, scipy.sparse.csc_matrix((subA2.shape[0], subA2.shape[0])))))).tocsr()
		LU=     scikits.sparse.cholmod.cholesky(M.tocsc(),mode='simplicial')
		print 'the sparsity ratio between Chol(M) and the  matrix M  is +'+str(LU.L().nnz/float(M.nnz))
		
		
		print 'connected components M'+str(scipy.sparse.csgraph.connected_components(M))
			
		#scipy.sparse.csgraph.connected_components(subA.T*subA)
		FactorConnections=subA*subA.T
		print 'connected components F'+str(scipy.sparse.csgraph.connected_components(FactorConnections))
		ST=scipy.sparse.csgraph.minimum_spanning_tree(FactorConnections)
		
	
	
	for idblock,mergegroupe in enumerate(mergegroupes):
		# find the indices of the variables used by the block
		idRows=np.hstack([np.arange(Aeq.blocks[g][0],Aeq.blocks[g][1]+1) for g in mergegroupe])
		subA=Aeq[idRows,:]
		t=np.array(np.abs(subA).sum(axis=0)).ravel()
		ids=np.nonzero(t)[0]
		list_block_ids.append(ids)
		# increment th number of time each variable is copied
		nb_used[ids]+=1
		subA2=subA[:,ids]
		# precompute the LU factorizartion of the matrix that needs to be inverted for the block
		M=scipy.sparse.vstack((scipy.sparse.hstack((gamma_ineq*scipy.sparse.eye(subA2.shape[1],subA2.shape[1]),subA2.T)),\
				       scipy.sparse.hstack((subA2, scipy.sparse.csc_matrix((subA2.shape[0], subA2.shape[0])))))).tocsr()
		if usesparseLU:
			
			ch.tic()
			LU=scipy.sparse.linalg.splu(M.tocsc())
			print ch.toc()
		else:
			ch.tic()
			LU=     scikits.sparse.cholmod.cholesky(M.tocsc(),mode='simplicial')
			factorization_duration= ch.toc()
			#A=     scikits.sparse.cholmod.analyze(M.tocsc(),mode='simplicial')
			#P=scipy.sparse.coo_matrix((np.ones(A.P().size),(A.P(),np.arange(A.P().size))))
			#permuted=P*M*P.T
			#plt.imshow(permuted.todense())
		# A.P()
			
			#LU=     scikits.sparse.cholmod.cholesky(M.tocsc(),mode='supernodal')# gives me matrix is not positive definite errors..
			
			print 'the sparsity ratio between Chol(M) and the  matrix M for block'+str(idblock)+' is +'+str(LU.L().nnz/float(M.nnz))+\
			' took '+str(factorization_duration)+ 'seconds to factorize'
			
			
			#LU.__=LU.solve_A
			#M2=convertToPySparseFormat(M)
			#LU = umfpack.factorize(M2, strategy="UMFPACK_STRATEGY_SYMMETRIC")
			#print "nnz per line :"+str(LU.nnz/float(M2.shape[0]) ) 
			
			
		xv.append(np.empty(M.shape[1],dtype=float))					
		luMs.append(LU )
		beqs.append(beq[idRows])
		pass
		
	
	

	def L(x,xp,lambda_ineq):
		E=c.dot(xp)
		for idblock in range(nb_blocks):
			diff=x[idblock]-xp[list_block_ids[idblock]]
			E+=0.5*gamma_ineq*np.sum((diff)**2)\
			+lambda_ineq[idblock].dot(diff)
		return E
	
	i=0
	
	x=[x0[list_block_ids[i]] for i in range(nb_blocks)]
	lambda_ineq=[np.zeros(list_block_ids[i].shape) for i in range(nb_blocks)]
	

	check=check_decrease(tol=1e-10)
	dpred=[np.zeros(list_block_ids[i].shape) for i in range(nb_blocks)]
	alpha=1.95#relaxation paramter should be in [0,2] , 1.95 seems to be often a good choice

	while i <=nb_iter:
		#solve the penalized problems with respect to each copy x		
		#print 'iter'+str(i)+' '+str(L(x, xp,lambda_ineq))
		#check.set(L(x, xp,lambda_ineq))
		for idblock in range(nb_blocks):
			y=np.hstack((gamma_ineq*xp[list_block_ids[idblock]]- lambda_ineq[idblock],beqs[idblock]))
			if usesparseLU:
				xv[idblock]=luMs[idblock].solve(y)	
			else:
				xv[idblock]=luMs[idblock].solve_A(y)	
				
				#luMs[idblock].solve(y,xv[idblock])
			x[idblock]=alpha*xv[idblock][:x[idblock].shape[0]]+(1-alpha)*xp[list_block_ids[idblock]]
			
			#check.add(L(x, xp,lambda_ineq))
		#print 'iter'+str(i)+' '+str(L(x, xp,lambda_ineq))		
		#solve the penalized problem with respect to xp	
		#c-sum_idblock  gamma_ineq*(x_[idblock]-xp[list_block_ids[idblock]])-lambda_ineq[idblock]=0
		xp[nb_used>0]=0
		for idblock in range(nb_blocks):
			xp[list_block_ids[idblock]]+=x[idblock]+lambda_ineq[idblock]/gamma_ineq # change formula here
		
		xp=xp-c/gamma_ineq 
		xp=xp/	np.maximum(nb_used,1)
		xp=np.maximum(xp,lb)
		xp=np.minimum(xp,ub)
		#check.add(L(x, xp,lambda_ineq))
		
		for idblock in range(nb_blocks):
			d=gamma_ineq*(x[idblock]-xp[list_block_ids[idblock]])
			#angle=np.sum(dpred[idblock]*d)/(np.sqrt(np.sum(dpred[idblock]**2))*+np.sqrt(np.sum(d**2)))
			#print angle
			#dpred[idblock]=d.copy()
			lambda_ineq[idblock]=lambda_ineq[idblock]+d
			#lambda_ineq[idblock]=lambda_ineq[idblock]+(1+max(0,angle))*d # trying some naive heuristic speedup but not working :(
			
			
		if i%nb_iter_plot==0:
					prev_elapsed=elapsed
					elapsed= (time.clock() - start)	
					if elapsed>max_time:
						break
					
					energy1=L(x, xp,lambda_ineq)
					energy2=energy1
					
					max_violated_equality=0
					max_violated_inequality=0
					print 'iter'+str(i)+": energy1= "+str(energy1) + " energy2="+str(energy2)+ ' elaspsed '+str(elapsed)+' second'+\
				          ' max violated inequality:'+str(max_violated_inequality)+\
				          ' max violated equality:'+str(max_violated_equality)
					if not callbackFunc is None:
						callbackFunc(i,(R* xp)[0:n],energy1,energy2,elapsed,max_violated_equality,max_violated_inequality)				
		i+=1
				
	return (R* xp)[0:n]


				
			
	
	


	

